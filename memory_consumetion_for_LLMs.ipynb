{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig"
      ],
      "metadata": {
        "id": "1zaBXa6nmMab"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"CohereForAI/c4ai-command-r-plus\"\n",
        "\n",
        "model_config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "hidden_layers = model_config.num_hidden_layers\n",
        "hidden_size = model_config.hidden_size\n",
        "attention_heads = model_config.num_attention_heads\n",
        "\n",
        "print(\"Model: \"+str(model_name))\n",
        "print(\"Hidden Layer (L): \"+str(hidden_layers))\n",
        "print(\"Hidden Size (h) : \"+str(hidden_size))\n",
        "print(\"Attention Heads (a): \"+str(attention_heads))"
      ],
      "metadata": {
        "id": "EAKDEWNVmXu3",
        "outputId": "b2285b61-06c0-4688-fa6d-9627dda01b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: CohereForAI/c4ai-command-r-plus\n",
            "Hidden Layer (L): 64\n",
            "Hidden Size (h) : 12288\n",
            "Attention Heads (a): 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of parameters in the model (in billions)\n",
        "nb_billion_parameters = 104 # @param {type:\"number\"}\n",
        "print(\"Number of parameters in the model (n): \"+str(nb_billion_parameters)+\"B\")\n",
        "\n",
        "#Precision of the parameters in the model\n",
        "bitwidth_model = 16 # @param {type:\"integer\"}\n",
        "print(\"Bitwidth of the model's parameters (p): \"+str(bitwidth_model)+\"-bit\")\n",
        "\n",
        "#Precision of the parameters in the optimizer\n",
        "bitwidth_optimizer = 32 # @param {type:\"integer\"}\n",
        "print(\"Bitwidth of the optimizer's parameters (o): \"+str(bitwidth_optimizer)+\"-bit\")\n",
        "\n",
        "#The maximum number of tokens in a sequence\n",
        "seqlen = 512 # @param {type:\"integer\"}\n",
        "print(\"Sequence length (s): \"+str(seqlen))\n",
        "\n",
        "#The batch size\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "print(\"Batch size (b): \"+str(batch_size))"
      ],
      "metadata": {
        "id": "DkC-rswlpo9N",
        "outputId": "98c3f16b-9bb6-4c2a-c9fb-f247da513ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the model (n): 104B\n",
            "Bitwidth of the model's parameters (p): 16-bit\n",
            "Bitwidth of the optimizer's parameters (o): 32-bit\n",
            "Sequence length (s): 512\n",
            "Batch size (b): 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_consumption():\n",
        "  #34 sbh + 5asÂ²b\n",
        "  return round((34*seqlen*batch_size*hidden_size + 5*attention_heads*seqlen*seqlen*batch_size)*2/(1024**3),2)\n",
        "\n",
        "def estimate_optimizer_size():\n",
        "  return round((2*nb_billion_parameters*bitwidth_optimizer/8*(1000**3))/(1024**3),2)\n",
        "\n",
        "def estimate_model_size():\n",
        "  return round(nb_billion_parameters*bitwidth_model/8*(1000**3)/(1024**3),2)\n",
        "\n",
        "activation_consumption = estimate_consumption()\n",
        "model_consumption = estimate_model_size()\n",
        "optimizer_consumption = estimate_optimizer_size()\n",
        "\n",
        "print(\"Memory consumption of the model: \"+str(model_consumption)+\" GB\\n\")\n",
        "\n",
        "print(\"Memory consumption of the optimizer: \"+str(optimizer_consumption)+\" GB\")\n",
        "print(\"Memory consumption of activations for fine-tuning: \"+str(activation_consumption*hidden_layers)+\" GB\")\n",
        "print(\"Total memory consumption for fine-tuning: \"+str(model_consumption+optimizer_consumption+activation_consumption*hidden_layers)+\" GB\\n\")\n",
        "\n",
        "print(\"Memory consumption of activations for inference: \"+str(activation_consumption)+\" GB\")\n",
        "print(\"Total memory consumption for inference: \"+str(model_consumption+activation_consumption)+\" GB\")\n",
        ""
      ],
      "metadata": {
        "id": "y2MEFdavreDK",
        "outputId": "13e7336b-45cd-4deb-bd51-77b134b6fbad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory consumption of the model: 193.72 GB\n",
            "\n",
            "Memory consumption of the optimizer: 774.86 GB\n",
            "Memory consumption of activations for fine-tuning: 323.84 GB\n",
            "Total memory consumption for fine-tuning: 1292.42 GB\n",
            "\n",
            "Memory consumption of activations for inference: 5.06 GB\n",
            "Total memory consumption for inference: 198.78 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9S7JEnIDsNze"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}